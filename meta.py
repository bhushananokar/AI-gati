# High-Accuracy Meta-Learning Implementation - Target 90-95% accuracy
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import transforms, datasets
import numpy as np
import os
import time
import random
from tqdm.notebook import tqdm
from collections import OrderedDict, deque
import copy
import matplotlib.pyplot as plt

def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

seed_everything()

# AGGRESSIVE Configuration for High Accuracy (90-95% target)
high_accuracy_config = {
    'darts_model_path': '/mnt/Test/SC202/trained_models/best_darts_model.pth',
    'data_dir': '/mnt/Test/SC202/IMG_CLASSES',
    'model_dir': '/mnt/Test/SC202/trained_models',
    
    # Model parameters
    'img_size': 224,
    'batch_size': 8,              # Smaller batches for stability
    
    # AGGRESSIVE: Much more conservative for high accuracy
    'meta_lr': 1e-5,              # Very small meta learning rate
    'inner_lr': 0.01,             # Larger inner LR for better adaptation
    'meta_epochs': 100,           # More epochs for convergence
    'num_inner_steps': 5,         # More adaptation steps
    'tasks_per_epoch': 15,        # More tasks for better learning
    'k_shot': 5,                  # More shots for easier learning
    'query_size': 5,              # Balanced query size
    'n_way': 2,                   # Start with binary classification
    'first_order': False,        # Use second-order gradients
    
    # High accuracy features
    'gradient_clip': 1.0,         # Less aggressive clipping
    'warmup_epochs': 15,          # Longer warmup
    'moving_avg_window': 10,      # Longer smoothing
    'early_stopping_patience': 25,
    'weight_decay': 1e-5,         # Less regularization
    'dropout_rate': 0.05,         # Minimal dropout
    
    # Learning rate schedule
    'use_cosine_schedule': True,
    'min_lr_factor': 0.01,
    
    'num_workers': 2,
    'use_cuda': True,
}

def load_data(data_dir, img_size=224, batch_size=32, num_workers=4):
    """Load data with better preprocessing for high accuracy"""
    print(f"Loading dataset from {data_dir}")
    
    train_dir = os.path.join(data_dir, "train")
    val_dir = os.path.join(data_dir, "val") 
    
    if not all(os.path.exists(d) for d in [train_dir, val_dir]):
        raise ValueError("Invalid dataset structure")
    
    # High-quality transforms for better accuracy
    train_transform = transforms.Compose([
        transforms.Resize((img_size + 56, img_size + 56)),  # Larger resize
        transforms.CenterCrop(img_size),                    # Center crop for consistency
        transforms.RandomHorizontalFlip(p=0.5),            # Standard flip
        transforms.RandomRotation(10),                      # Light rotation
        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize((img_size + 56, img_size + 56)),
        transforms.CenterCrop(img_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)
    val_dataset = datasets.ImageFolder(root=val_dir, transform=val_transform)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, 
                             num_workers=num_workers, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, 
                           num_workers=num_workers, pin_memory=True)
    
    num_classes = len(train_dataset.classes)
    class_names = train_dataset.classes
    
    print(f"Dataset loaded with {num_classes} classes: {class_names}")
    return train_loader, val_loader, num_classes, class_names

# Much simpler ConvNeXt for high accuracy
class HighAccuracyConvNeXtBlock(nn.Module):
    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):
        super().__init__()
        # Simplified block for better learning
        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim, bias=False)
        self.norm = nn.LayerNorm(dim, eps=1e-6)
        self.pwconv1 = nn.Linear(dim, 2 * dim, bias=False)  # Smaller expansion
        self.act = nn.GELU()
        self.pwconv2 = nn.Linear(2 * dim, dim, bias=False)
        
        # Conservative layer scale
        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), 
                                  requires_grad=True)
        self.drop_path = nn.Identity()  # Remove drop path for stability

    def forward(self, x):
        input_x = x
        x = self.dwconv(x)
        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)
        x = self.norm(x)
        x = self.pwconv1(x)
        x = self.act(x)
        x = self.pwconv2(x)
        x = self.gamma * x
        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)
        x = input_x + x  # Simple residual
        return x

class LayerNormChannelsFirst(nn.Module):
    def __init__(self, normalized_shape, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.eps = eps
    
    def forward(self, x):
        u = x.mean(1, keepdim=True)
        s = (x - u).pow(2).mean(1, keepdim=True)
        x = (x - u) / torch.sqrt(s + self.eps)
        x = self.weight[:, None, None] * x + self.bias[:, None, None]
        return x

class HighAccuracyConvNeXt(nn.Module):
    """Ultra-simplified ConvNeXt for high meta-learning accuracy"""
    def __init__(self, in_chans=3, num_classes=2, 
                 depths=[1, 1, 2, 1], dims=[48, 96, 192, 384]):  # Much smaller
        super().__init__()
        
        self.depths = depths
        self.dims = dims
        self.num_classes = num_classes
        
        # Simple stem
        self.downsample_layers = nn.ModuleList()
        stem = nn.Sequential(
            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4, bias=False),
            LayerNormChannelsFirst(dims[0])
        )
        self.downsample_layers.append(stem)
        
        # Downsampling layers
        for i in range(3):
            downsample_layer = nn.Sequential(
                LayerNormChannelsFirst(dims[i]),
                nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2, bias=False),
            )
            self.downsample_layers.append(downsample_layer)
        
        # Very simple stages
        self.stages = nn.ModuleList()
        for i in range(4):
            stage_blocks = []
            for j in range(depths[i]):
                block = HighAccuracyConvNeXtBlock(
                    dim=dims[i], 
                    drop_path=0.0,  # No drop path
                    layer_scale_init_value=1e-6
                )
                stage_blocks.append(block)
            self.stages.append(nn.ModuleList(stage_blocks))
        
        # Simple classifier
        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)
        self.head = nn.Linear(dims[-1], num_classes, bias=True)  # Add bias
        
        self.apply(self._init_weights)
    
    def _init_weights(self, m):
        if isinstance(m, (nn.Conv2d, nn.Linear)):
            nn.init.trunc_normal_(m.weight, std=0.02)
            if hasattr(m, 'bias') and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, (nn.LayerNorm, LayerNormChannelsFirst)):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
    
    def forward_features(self, x):
        for i in range(4):
            x = self.downsample_layers[i](x)
            for block in self.stages[i]:
                x = block(x)
        
        # Global average pooling
        x = x.mean([-2, -1])
        x = self.norm(x)
        return x
    
    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x)
        return x
    
    def clone(self):
        """Create a deep copy for inner loop"""
        clone = HighAccuracyConvNeXt(
            in_chans=3,
            num_classes=self.num_classes,
            depths=self.depths,
            dims=self.dims
        )
        clone.load_state_dict(self.state_dict())
        return clone

def create_high_quality_tasks(data_loader, num_tasks, n_way=2, k_shot=5, query_size=5, seed=None):
    """Create high-quality tasks with better class separation"""
    if seed is not None:
        random.seed(seed)
        np.random.seed(seed)
    
    print("Collecting data by class with quality filtering...")
    class_data = {}
    
    # Collect more samples per class
    for inputs, labels in tqdm(data_loader, desc="Loading data", leave=False):
        for i, label in enumerate(labels):
            label_item = label.item()
            if label_item not in class_data:
                class_data[label_item] = []
            
            # Collect more samples for better task quality
            if len(class_data[label_item]) < (k_shot + query_size + 15):
                class_data[label_item].append(inputs[i].clone())
    
    # Filter classes with sufficient high-quality data
    min_samples = k_shot + query_size + 5  # Extra buffer
    valid_classes = [c for c, data in class_data.items() if len(data) >= min_samples]
    
    print(f"Found {len(valid_classes)} classes with ‚â•{min_samples} samples")
    
    if len(valid_classes) < n_way:
        raise ValueError(f"Need ‚â•{n_way} classes, only found {len(valid_classes)}")
    
    tasks = []
    for task_idx in range(num_tasks):
        # Sample well-separated classes (avoid too similar classes)
        task_classes = np.random.choice(valid_classes, size=n_way, replace=False)
        
        support_data, support_labels = [], []
        query_data, query_labels = [], []
        
        for new_label, original_class in enumerate(task_classes):
            available_data = class_data[original_class]
            
            # Sample more conservatively for quality
            total_needed = k_shot + query_size
            available_count = len(available_data)
            
            if available_count >= total_needed:
                indices = np.random.choice(
                    available_count, 
                    size=total_needed, 
                    replace=False
                )
                
                # Support set
                for i in range(k_shot):
                    support_data.append(available_data[indices[i]])
                    support_labels.append(new_label)
                
                # Query set
                for i in range(k_shot, total_needed):
                    query_data.append(available_data[indices[i]])
                    query_labels.append(new_label)
        
        # Only add high-quality tasks
        if len(support_data) == n_way * k_shot and len(query_data) == n_way * query_size:
            support_data = torch.stack(support_data)
            support_labels = torch.tensor(support_labels)
            query_data = torch.stack(query_data)
            query_labels = torch.tensor(query_labels)
            
            tasks.append((support_data, support_labels, query_data, query_labels))
    
    print(f"Created {len(tasks)} high-quality tasks")
    return tasks

class HighAccuracyMAML:
    """MAML optimized for high accuracy (90-95% target)"""
    def __init__(self, model, inner_lr=0.01, meta_lr=1e-5, num_inner_steps=5, 
                 gradient_clip=1.0, warmup_epochs=15, weight_decay=1e-5, first_order=False):
        self.model = model
        self.inner_lr = inner_lr
        self.meta_lr = meta_lr
        self.base_meta_lr = meta_lr
        self.num_inner_steps = num_inner_steps
        self.gradient_clip = gradient_clip
        self.warmup_epochs = warmup_epochs
        self.first_order = first_order
        
        # High-accuracy optimizer
        self.meta_optimizer = torch.optim.AdamW(
            self.model.parameters(), 
            lr=meta_lr,
            weight_decay=weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # Conservative scheduler
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.meta_optimizer, T_max=100, eta_min=meta_lr/100
        )
        
        # Extended tracking
        self.loss_history = deque(maxlen=200)
        self.acc_history = deque(maxlen=200)
        
    def get_lr_scale(self, epoch):
        """Very gradual warmup for stability"""
        if epoch < self.warmup_epochs:
            return 0.05 + 0.95 * (epoch / self.warmup_epochs)  # Start at 5% LR
        return 1.0
    
    def inner_loop(self, support_data, support_labels, criterion, device):
        """High-quality inner loop with more adaptation steps"""
        fast_model = self.model.clone().to(device)
        fast_model.train()
        
        inner_losses = []
        inner_accuracies = []
        
        for step in range(self.num_inner_steps):
            outputs = fast_model(support_data)
            loss = criterion(outputs, support_labels)
            inner_losses.append(loss.item())
            
            # Track inner accuracy
            with torch.no_grad():
                _, preds = torch.max(outputs, 1)
                acc = (preds == support_labels).float().mean()
                inner_accuracies.append(acc.item())
            
            # Compute gradients
            gradients = torch.autograd.grad(
                loss, fast_model.parameters(),
                create_graph=not self.first_order,
                retain_graph=not self.first_order,
                allow_unused=True
            )
            
            # High-quality gradient updates
            with torch.no_grad():
                for param, grad in zip(fast_model.parameters(), gradients):
                    if grad is not None:
                        # Adaptive gradient clipping
                        grad_norm = grad.norm()
                        if grad_norm > self.gradient_clip:
                            grad = grad * (self.gradient_clip / grad_norm)
                        param.subtract_(self.inner_lr * grad)
        
        return fast_model, inner_losses, inner_accuracies
    
    def meta_step(self, batch_tasks, criterion, device, epoch=0):
        """High-accuracy meta step"""
        self.model.train()
        meta_losses = []
        task_accuracies = []
        inner_losses_all = []
        inner_accuracies_all = []
        
        lr_scale = self.get_lr_scale(epoch)
        
        successful_tasks = 0
        
        for task_idx, (support_data, support_labels, query_data, query_labels) in enumerate(batch_tasks):
            try:
                support_data = support_data.to(device)
                support_labels = support_labels.to(device)
                query_data = query_data.to(device)
                query_labels = query_labels.to(device)
                
                # High-quality inner loop
                fast_model, inner_losses, inner_accs = self.inner_loop(
                    support_data, support_labels, criterion, device
                )
                inner_losses_all.extend(inner_losses)
                inner_accuracies_all.extend(inner_accs)
                
                # Query evaluation
                fast_model.eval()
                with torch.set_grad_enabled(True):
                    query_outputs = fast_model(query_data)
                    query_loss = criterion(query_outputs, query_labels)
                
                meta_losses.append(query_loss)
                
                # Calculate accuracy
                with torch.no_grad():
                    _, preds = torch.max(query_outputs, 1)
                    accuracy = (preds == query_labels).float().mean()
                    task_accuracies.append(accuracy.item())
                
                successful_tasks += 1
                
                # Debug info for first few epochs
                if epoch < 5:
                    print(f"  Task {task_idx}: Inner acc {inner_accs[-1]:.3f} ‚Üí Query acc {accuracy:.3f}")
                
            except Exception as e:
                print(f"Error in task {task_idx}: {e}")
                continue
        
        if successful_tasks > 0:
            meta_loss = torch.stack(meta_losses).mean()
            
            # Meta optimization
            self.meta_optimizer.zero_grad()
            meta_loss.backward()
            
            # High-quality gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)
            
            # Apply learning rate scaling
            for param_group in self.meta_optimizer.param_groups:
                param_group['lr'] = self.base_meta_lr * lr_scale
            
            self.meta_optimizer.step()
            
            # Track metrics
            avg_accuracy = np.mean(task_accuracies) if task_accuracies else 0
            avg_inner_loss = np.mean(inner_losses_all) if inner_losses_all else 0
            avg_inner_acc = np.mean(inner_accuracies_all) if inner_accuracies_all else 0
            
            self.loss_history.append(meta_loss.item())
            self.acc_history.append(avg_accuracy)
            
            return meta_loss.item(), avg_accuracy, avg_inner_loss, avg_inner_acc
        
        return 0.0, 0.0, 0.0, 0.0
    
    def get_smoothed_metrics(self, window=10):
        """Get smoothed metrics with longer window"""
        if len(self.loss_history) > 0 and len(self.acc_history) > 0:
            smooth_loss = np.mean(list(self.loss_history)[-window:])
            smooth_acc = np.mean(list(self.acc_history)[-window:])
            return smooth_loss, smooth_acc
        return 0.0, 0.0

def run_high_accuracy_meta_learning(config):
    """Meta-learning optimized for 90-95% accuracy"""
    device = torch.device('cuda' if config['use_cuda'] and torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    print("üéØ TARGET: 90-95% accuracy meta-learning")
    
    # Load data
    train_loader, val_loader, num_classes, class_names = load_data(
        data_dir=config['data_dir'],
        img_size=config['img_size'],
        batch_size=config['batch_size'],
        num_workers=config['num_workers']
    )
    
    # Create ultra-simple ConvNeXt model
    model = HighAccuracyConvNeXt(
        in_chans=3,
        num_classes=config['n_way'],
        depths=[1, 1, 2, 1],  # Very simple
        dims=[48, 96, 192, 384]  # Small but sufficient
    ).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Ultra-simple ConvNeXt parameters: {total_params:,}")
    
    # Initialize high-accuracy MAML
    maml = HighAccuracyMAML(
        model=model,
        inner_lr=config['inner_lr'],
        meta_lr=config['meta_lr'],
        num_inner_steps=config['num_inner_steps'],
        gradient_clip=config['gradient_clip'],
        warmup_epochs=config['warmup_epochs'],
        weight_decay=config['weight_decay'],
        first_order=config['first_order']
    )
    
    criterion = nn.CrossEntropyLoss()
    best_accuracy = 0.0
    patience_counter = 0
    
    # Training history
    history = {
        'meta_loss': [],
        'task_acc': [],
        'smoothed_loss': [],
        'smoothed_acc': [],
        'inner_acc': []
    }
    
    print(f"Starting HIGH-ACCURACY meta-learning for {config['meta_epochs']} epochs")
    print(f"Config: {config['n_way']}-way, {config['k_shot']}-shot, {config['query_size']} queries")
    print(f"Inner steps: {config['num_inner_steps']}, Tasks per epoch: {config['tasks_per_epoch']}")
    
    # Training loop
    for epoch in range(config['meta_epochs']):
        epoch_start = time.time()
        print(f"\n=== Epoch {epoch+1}/{config['meta_epochs']} ===")
        
        # Create high-quality tasks
        train_tasks = create_high_quality_tasks(
            train_loader,
            num_tasks=config['tasks_per_epoch'],
            n_way=config['n_way'],
            k_shot=config['k_shot'],
            query_size=config['query_size'],
            seed=epoch
        )
        
        if len(train_tasks) == 0:
            print("No valid tasks created, skipping epoch")
            continue
        
        # Meta training step
        meta_loss, task_acc, inner_loss, inner_acc = maml.meta_step(
            train_tasks, criterion, device, epoch
        )
        
        # Get smoothed metrics
        smooth_loss, smooth_acc = maml.get_smoothed_metrics(
            window=config['moving_avg_window']
        )
        
        # Record history
        history['meta_loss'].append(meta_loss)
        history['task_acc'].append(task_acc)
        history['smoothed_loss'].append(smooth_loss)
        history['smoothed_acc'].append(smooth_acc)
        history['inner_acc'].append(inner_acc)
        
        # Step scheduler after warmup
        if epoch >= config['warmup_epochs']:
            maml.scheduler.step()
        
        current_lr = maml.meta_optimizer.param_groups[0]['lr']
        epoch_time = time.time() - epoch_start
        
        # Comprehensive logging
        print(f"Epoch {epoch+1} ({epoch_time:.1f}s):")
        print(f"  üìä Raw - Loss: {meta_loss:.4f}, Acc: {task_acc:.4f}")
        print(f"  üìà Smoothed - Loss: {smooth_loss:.4f}, Acc: {smooth_acc:.4f}")
        print(f"  üîÑ Inner - Loss: {inner_loss:.4f}, Acc: {inner_acc:.4f}")
        print(f"  ‚öôÔ∏è  LR: {current_lr:.6f}")
        
        # Check if we're hitting our target
        if smooth_acc >= 0.90:
            print(f"  üéØ HIGH ACCURACY ACHIEVED: {smooth_acc:.4f} (‚â•90%)")
        elif smooth_acc >= 0.80:
            print(f"  ‚úÖ Good progress: {smooth_acc:.4f} (‚â•80%)")
        elif smooth_acc >= 0.70:
            print(f"  üìç Moderate progress: {smooth_acc:.4f} (‚â•70%)")
        else:
            print(f"  üîÑ Still learning: {smooth_acc:.4f} (<70%)")
        
        # Early stopping based on smoothed accuracy
        if smooth_acc > best_accuracy:
            best_accuracy = smooth_acc
            patience_counter = 0
            
            # Save best model
            torch.save({
                'model_state_dict': model.state_dict(),
                'epoch': epoch,
                'accuracy': smooth_acc,
                'config': config,
                'history': history,
                'best_accuracy': best_accuracy
            }, os.path.join(config['model_dir'], 'best_high_accuracy_convnext_meta.pth'))
            print(f"  üíæ New best accuracy: {smooth_acc:.4f}")
        else:
            patience_counter += 1
            print(f"  ‚è± No improvement for {patience_counter} epochs")
            
        # Early stopping
        if patience_counter >= config['early_stopping_patience']:
            print(f"\nüõë Early stopping after {patience_counter} epochs without improvement")
            break
        
        # Plot progress every 20 epochs
        if (epoch + 1) % 20 == 0:
            plt.figure(figsize=(15, 5))
            
            plt.subplot(1, 3, 1)
            plt.plot(history['meta_loss'], alpha=0.3, label='Raw Loss')
            plt.plot(history['smoothed_loss'], label='Smoothed Loss', linewidth=2)
            plt.title('Meta Loss')
            plt.legend()
            plt.grid(True)
            
            plt.subplot(1, 3, 2)
            plt.plot(history['task_acc'], alpha=0.3, label='Raw Accuracy')
            plt.plot(history['smoothed_acc'], label='Smoothed Accuracy', linewidth=2)
            plt.axhline(y=0.9, color='r', linestyle='--', label='90% Target')
            plt.axhline(y=0.95, color='g', linestyle='--', label='95% Target')
            plt.title('Task Accuracy')
            plt.legend()
            plt.grid(True)
            
            plt.subplot(1, 3, 3)
            plt.plot(history['inner_acc'], label='Inner Loop Accuracy', linewidth=2)
            plt.title('Inner Loop Learning')
            plt.legend()
            plt.grid(True)
            
            plt.tight_layout()
            plt.savefig(os.path.join(config['model_dir'], f'high_acc_progress_epoch_{epoch+1}.png'))
            plt.show()
    
    print(f"\nüèÅ High-Accuracy Meta-Learning completed!")
    print(f"üéØ Best accuracy achieved: {best_accuracy:.4f}")
    
    if best_accuracy >= 0.90:
        print("üéâ SUCCESS: Achieved 90%+ accuracy target!")
    elif best_accuracy >= 0.80:
        print("‚úÖ Good result: 80%+ accuracy achieved")
    else:
        print("üîÑ Consider: Longer training or simpler tasks")
    
    return model, history

# Run the high-accuracy meta-learning
print("üöÄ Starting HIGH-ACCURACY ConvNeXt Meta-Learning")
print("Target: 90-95% accuracy")
model, history = run_high_accuracy_meta_learning(high_accuracy_config)
